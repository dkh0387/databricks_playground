{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54bf7783-baa0-4415-b783-9037168afac0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9993d460-a234-4818-a87b-5c8d0cd295f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Install the required packages from the specified requirements file\n",
    "os.system(\"pip install -r https://raw.githubusercontent.com/George-Michael-Dagogo/World_news_tutorial/main/requirements.txt\")\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "597eed4a-d240-40a7-b639-55e2df3f50ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. Load articles from News API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fd968d9-2719-4ed5-937f-d7e0ebc9fc7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from newsapi.newsapi_client import NewsApiClient\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from datetime import date, timedelta\n",
    "\n",
    "def extract_transform_function():\n",
    "    today = date.today()\n",
    "    # Get today's date\n",
    "    yesterday = today - timedelta(days = 1)\n",
    "    # Get yesterday's date\n",
    "    day_before_yesterday = today - timedelta(days = 2)\n",
    "    # Get the day before yesterday's date\n",
    "\n",
    "    # Initialize the News API client with an API key\n",
    "    newsapi = NewsApiClient(api_key='ff4373852c2343a98303951439854f8c')\n",
    "\n",
    "    # Get top headlines for the entertainment category in English, with a page size of 90\n",
    "    top_headlines = newsapi.get_top_headlines(   \n",
    "                                            category='entertainment',\n",
    "                                            language='en',\n",
    "                                            page_size = 90,\n",
    "                                            page= 1)\n",
    "\n",
    "    # Extract articles from the API response\n",
    "    articles = top_headlines.get('articles',[])\n",
    "\n",
    "    # Create a DataFrame from the articles, selecting specific columns\n",
    "    init_df = pd.DataFrame(articles, columns = ['source','title','publishedAt','author','url'])\n",
    "\n",
    "    # Extract the 'name' field from the 'source' dictionary in each row\n",
    "    init_df['source'] = init_df['source'].apply(lambda x: x['name'] if pd.notna(x) and 'name' in x else None)\n",
    "\n",
    "    # Convert 'publishedAt' to datetime format\n",
    "    init_df['publishedAt'] = pd.to_datetime(init_df['publishedAt'])\n",
    "\n",
    "    # Filter the DataFrame for articles published on the day before yesterday or yesterday\n",
    "    filtered_df = init_df[(init_df['publishedAt'].dt.date == day_before_yesterday) | (init_df['publishedAt'].dt.date == yesterday)]\n",
    "    # Rename the 'publishedAt' column to 'date_posted'\n",
    "    filtered_df.rename(columns={'publishedAt': 'date_posted'}, inplace=True)\n",
    "\n",
    "    # Make a copy of the filtered DataFrame\n",
    "    df = filtered_df.copy()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "387d3c9b-aa11-4a4c-9f9a-2a37310dad50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3. Parse articles content from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3193ce18-5cde-4ebf-8bcf-b1c90cd7c017",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Function to parse article content from url\n",
    "def full_content(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        article_text = ' '.join([p.text for p in soup.find_all('p')])\n",
    "        return article_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving content from {url}: {e}\")\n",
    "        return 'couldnt retrieve'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81576b24-8dac-4851-98a8-0adb62c3f29d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 4. Metrics for article content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22eb4f3b-3376-4592-936c-038e394d2f49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.1 Count words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca8e726d-0f31-4e03-89d9-ad32a625929c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to count words in a text excluding stopwords\n",
    "def count_words_without_stopwords(text):\n",
    "    if isinstance(text, (str, bytes)):\n",
    "        words = nltk.word_tokenize(str(text))\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "        return len(filtered_words)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "777cd0f5-f23e-47f9-ab59-607527428274",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.2 Get sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "650baef5-626a-4cfd-9ea0-f7787e5d7248",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to get sentiment and compound score for a given text\n",
    "def get_sentiment(row, sid):\n",
    "    sentiment_scores = sid.polarity_scores(row)\n",
    "    compound_score = sentiment_scores['compound']\n",
    "\n",
    "    if compound_score >= 0.05:\n",
    "        sentiment = 'Positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        sentiment = 'Negative'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "\n",
    "    return sentiment, compound_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a55b0b03-c2ab-4035-8e2d-395b511e409c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 5. Run and create a result DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "341c4171-efdf-4c70-9ba8-6cae7201ebfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    df = extract_transform_function()\n",
    "    #display(dataframe)\n",
    "\n",
    "    # Apply the full_content function to each URL in the DataFrame\n",
    "    df['content'] = df['url'].apply(full_content)\n",
    "    # Replace newlines in the 'content' column with spaces\n",
    "    df['content'] = df['content'].str.replace('\\n', ' ')\n",
    "    # Filter out rows where the content could not be retrieved\n",
    "    df = df[df['content'] != 'couldnt retrieve']\n",
    "\n",
    "    # Download the NLTK stopwords dataset and other required datasets\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "    # Apply the word count function to the 'content' column\n",
    "    df['word_count'] = df['content'].apply(count_words_without_stopwords)\n",
    "\n",
    "    # Download the VADER sentiment analysis lexicon\n",
    "    nltk.download('vader_lexicon')\n",
    "\n",
    "    # Initialize the SentimentIntensityAnalyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Apply the sentiment analysis function to the 'content' column\n",
    "    df[['sentiment', 'compound_score']] = df['content'].astype(str).apply(lambda x: pd.Series(get_sentiment(x, sid)))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec80cdba-0ea5-4711-b8c6-1f81c94f0ac4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "771e9b81-689f-4063-9b98-3f93c57fe7ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "defda7fa-ef66-4a51-a1c5-90386fac592b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 6. Store data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8663ae61-4d9f-4248-8aae-68a6c1442c28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6.1 Create schema and table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23ceba4e-1af0-4c23-8258-a52ec551e1cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " %sql\n",
    "CREATE DATABASE IF NOT EXISTS the_news;\n",
    "CREATE TABLE IF NOT EXISTS the_news.news_table (\n",
    "source STRING,\n",
    "title STRING,\n",
    "date_posted DATE,\n",
    "author STRING,\n",
    "url STRING,\n",
    "content STRING,\n",
    "word_count INT,\n",
    "sentiment STRING,\n",
    "compound_score DOUBLE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e17413cd-e312-45db-b470-611faf8d7a3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6.2 Write data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ec05d2e-6b09-44d2-8655-0804475aca09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, IntegerType, DoubleType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CreateTableExample\").getOrCreate()\n",
    "\n",
    "# Define the schema explicitly if necessary\n",
    "schema = StructType([\n",
    "    StructField(\"source\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"date_posted\", DateType(), True), \n",
    "    StructField(\"author\", StringType(), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    StructField(\"content\", StringType(), True),\n",
    "    StructField(\"word_count\", IntegerType(), True),\n",
    "    StructField(\"sentiment\", StringType(), True),\n",
    "    StructField(\"compound_score\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "spark_df = spark.createDataFrame(df, schema=schema)\n",
    "spark_df.write.mode('append').saveAsTable('the_news.news_table')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7546329909270954,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "News example",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
